{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "0JNfMoWRLTvX"
   },
   "source": [
    "![Roboflow banner](https://media.roboflow.com/banner.jpeg?updatedAt=1682523622384)\n",
    "\n",
    "# Roboflow Model Evaluation üîé\n",
    "\n",
    "[Roboflow Evaluations](https://github.com/roboflow/evaluations) is a framework for evaluating the results of computer vision models. Think OpenAI Evals, but for computer vision models.\n",
    "\n",
    "Using Evaluations, you can\n",
    "\n",
    "1. Evaluate the difference between ground truth (your annotated data) and predictions from your [Roboflow models](https://roboflow.com). You can use this information to better understand the quality of predictions from your model and areas where improvement is needed.\n",
    "\n",
    "2. Evaluate ground truth against results from a zero-shot model with a text input. [Grounding DINO](https://github.com/IDEA-Research/GroundingDINO) and [CLIP](https://github.com/openai/clip) are supported.\n",
    "\n",
    "## Steps in this Tutorial\n",
    "\n",
    "In this tutorial, we are going to cover:\n",
    "\n",
    "- How to set up Roboflow Evaluations with a Roboflow model, and;\n",
    "- How to run ground truth / Roboflow prediction analysis on an existing model.\n",
    "\n",
    "By the end of this guide, we will have a confusion matrix like the one below, as well as the following statistics:\n",
    "\n",
    "- Precision\n",
    "- Accuracy\n",
    "- F1 Score\n",
    "\n",
    "Without further ado, let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create a Data Loader üóÉÔ∏è\n",
    "\n",
    "Evaluations uses ground truth data from either:\n",
    "\n",
    "1. An existing Roboflow model, or;\n",
    "2. A JSON file that contains ground truth mapped to file names (see the evaluations.dataloaders.JSONDataLoader class docstrings for more information on how to compose this file).\n",
    "\n",
    "In this example, we will evaluate a model in Roboflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mFGq_r3POafL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/james/src/clip/venv/lib/python3.9/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (5.1.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from evaluations.dataloaders import (RoboflowDataLoader, RoboflowPredictionsDataLoader)\n",
    "from evaluations.roboflow import RoboflowEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Evaluator üíª\n",
    "\n",
    "An Evaluator uses a model to run inference on data in a dataset. This data run through the model. Inference results are compared to the ground truth from the provided data.\n",
    "\n",
    "Confusion matrices and ground truth vs. inference result visualizations are created for each image on which inference is run, saved in `output/matrices` and `output/images/` respectively.\n",
    "\n",
    "In the code below, we will create an evaluator that uses the aforementioned Roboflow model that we initialized and the data we collected from the Roboflow API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are already logged into Roboflow. To make a different login, run roboflow.login(force=True).\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    }
   ],
   "source": [
    "class_names, ground_truth, model = RoboflowDataLoader(\n",
    "    workspace_url=\"james-gallagher-87fuq\",\n",
    "    project_url=\"mug-detector-eocwp\",\n",
    "    project_version=12,\n",
    "    image_files=\"/Users/james/src/clip/model_eval/dataset-new\",\n",
    ").download_dataset()\n",
    "\n",
    "predictions = RoboflowPredictionsDataLoader(\n",
    "    model=model,\n",
    "    model_type=\"object-detection\",\n",
    "    image_files=\"/Users/james/src/clip/model_eval/dataset-new/\",\n",
    "    class_names=class_names,\n",
    ").process_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Analysis üìä\n",
    "\n",
    "The following lines of code will plot an aggregate confusion matrix representing the results of inference from all images in your dataset and display it in this notebook.\n",
    "\n",
    "After showing the confusion matrix, we will calculate and display precision, recall, and f1 score associated with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new/valid/images/IMG_1549-Large_jpeg_jpg.rf.21ff07dae783140d021c11ded2304576.jpg ... ['cup', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new/valid/images/IMG_1511-Large_jpeg_jpg.rf.b230e98b40b8ef4ece04e375dce34d75.jpg ... ['cup', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new/valid/images/IMG_4769_JPG_jpg.rf.a6774c9040a1adbe5652b28199552b9a.jpg ... ['cup', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new/valid/images/IMG_1544-Large_jpeg_jpg.rf.3d49ae7cd57da2930617648df0ba1df3.jpg ... ['cup', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new/valid/images/IMG_1540-Large_jpeg_jpg.rf.c14bd974a669f3a3cd4f8a4f2cf6b140.jpg ... ['cup', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new/valid/images/IMG_1381-Large_jpeg_jpg.rf.e1d3218b9982b0be91066b9d0864404f.jpg ... ['cup', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new/valid/images/IMG_5165-Large_jpeg_jpg.rf.505a60d787bbaef4a26ede6c8948b866.jpg ... ['cup', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new/valid/images/IMG_1502-Large_jpeg_jpg.rf.ce9aaef587d8e67cc1a5b77bdc427411.jpg ... ['cup', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new/valid/images/IMG_1518-Large_jpeg_jpg.rf.d4e63b3e76ce4b27fbaa440d0c5613ef.jpg ... ['cup', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new/valid/images/IMG_5028-Large_jpeg_jpg.rf.7f4fde50208759e9b1bc3427c2560588.jpg ... ['cup', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new/valid/images/IMG_1564-Large_jpeg_jpg.rf.08aa469c4a934cd7fe28112a5a2a3918.jpg ... ['cup', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new/valid/images/IMG_1554-Large_jpeg_jpg.rf.1ddcbd8d9a1d42119a6f46d046521a23.jpg ... ['cup', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new/valid/images/IMG_0190-Large_jpeg_jpg.rf.4c8bdd1aa008755e5e169c6fdf70d8c9.jpg ... ['cup', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new/valid/images/2010DC3A-1E7A-46AE-BC81-5234DE0D2E08-Large_jpeg_jpg.rf.deb808962eb3b74cae3842deab8b0c40.jpg ... ['cup', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new/valid/images/75DE96AE-DD42-497C-8EAE-C81EEFFB877E-Large_jpeg_jpg.rf.85bcbb2e18d96bd707e38bcefde4efdb.jpg ... ['cup', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new/valid/images/IMG_0362-Large_jpeg_jpg.rf.928b62536cf6ab99b3e1d7b76e1e7def.jpg ... ['cup', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new/valid/images/IMG_1572-Large_jpeg_jpg.rf.653d09adf1b3fc0f9e7dae6edb674b5e.jpg ... ['cup', 'background']\n",
      "Precision: 1.0\n",
      "Recall: 0.5\n",
      "f1 Score: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "evaluator = RoboflowEvaluator(\n",
    "    ground_truth=ground_truth, predictions=predictions, class_names=class_names, mode=\"batch\"\n",
    ")\n",
    "\n",
    "cf = evaluator.eval_model_predictions()\n",
    "\n",
    "data = evaluator.calculate_statistics()\n",
    "\n",
    "print(\"Precision:\", data.precision)\n",
    "print(\"Recall:\", data.recall)\n",
    "print(\"f1 Score:\", data.f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Prompts\n",
    "\n",
    "You can use the `CompareEvaluations` class to run multiple evaluations and return the class associated with the best performing model.\n",
    "\n",
    "Note: The `CompareEvaluations` class takes in single class names.\n",
    "\n",
    "In the example below, we will compare two prompts against CLIP to find out which prompt most effectively classifies our data.\n",
    "\n",
    "We will work with a dataset of apples. The sample size is 10 so inference should not take too long.\n",
    "\n",
    "In the code cell below, we will load the dataset with which we will be working in our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are already logged into Roboflow. To make a different login, run roboflow.login(force=True).\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/valid/apple/000325191_jpg.rf.8e478cb228cb5e7a7b17b14e26466968.jpg ... ['orange', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/valid/apple/apple-fuji-1-kg-product-images-o590000001-p590000001-0-202203151906_jpg.rf.f518348005b1d18b089c26bc018a02d2.jpg ... ['orange', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/test/apple/images-1-_jpg.rf.23bed4416b487cf1419b2761f3b6a492.jpg ... ['orange', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/train/apple/GP_74888233_GP_L_jpg.rf.c016cba8bc070a691955b37c0df96b02.jpg ... ['orange', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/train/apple/images_jpg.rf.b21a1b7b5a0ea2c68ac88ca41aa32235.jpg ... ['orange', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/train/apple/20Ounce_NYAS-Apples2_png.rf.3541f74ff8d85a2aeafea45ec366cd4d.jpg ... ['orange', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/train/apple/6000200094512_jpg.rf.dfadbaf4370c70e66fe44304dd9555f3.jpg ... ['orange', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/train/apple/Apple_jpg.rf.30a95a91386b71de8d72ae95b1999f0c.jpg ... ['orange', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/train/apple/apple-fuji-exoticfruitscouk-982233_jpg.rf.c435c84992101c99136132ff25c52a61.jpg ... ['orange', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/train/apple/hot81z-1024x866_jpg.rf.998bd30fb9d0d62bccd3cd0295db1eb8.jpg ... ['orange', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/valid/apple/000325191_jpg.rf.8e478cb228cb5e7a7b17b14e26466968.jpg ... ['red apple', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/valid/apple/apple-fuji-1-kg-product-images-o590000001-p590000001-0-202203151906_jpg.rf.f518348005b1d18b089c26bc018a02d2.jpg ... ['red apple', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/test/apple/images-1-_jpg.rf.23bed4416b487cf1419b2761f3b6a492.jpg ... ['red apple', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/train/apple/GP_74888233_GP_L_jpg.rf.c016cba8bc070a691955b37c0df96b02.jpg ... ['red apple', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/train/apple/images_jpg.rf.b21a1b7b5a0ea2c68ac88ca41aa32235.jpg ... ['red apple', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/train/apple/20Ounce_NYAS-Apples2_png.rf.3541f74ff8d85a2aeafea45ec366cd4d.jpg ... ['red apple', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/train/apple/6000200094512_jpg.rf.dfadbaf4370c70e66fe44304dd9555f3.jpg ... ['red apple', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/train/apple/Apple_jpg.rf.30a95a91386b71de8d72ae95b1999f0c.jpg ... ['red apple', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/train/apple/apple-fuji-exoticfruitscouk-982233_jpg.rf.c435c84992101c99136132ff25c52a61.jpg ... ['red apple', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/train/apple/hot81z-1024x866_jpg.rf.998bd30fb9d0d62bccd3cd0295db1eb8.jpg ... ['red apple', 'background']\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "f1 Score: 1.0\n",
      "Class Name: red apple\n"
     ]
    }
   ],
   "source": [
    "from evaluations.clip import CLIPEvaluator\n",
    "from evaluations.dataloaders import RoboflowDataLoader\n",
    "from evaluations.dataloaders.cliploader import CLIPDataLoader\n",
    "from evaluations import CompareEvaluations\n",
    "import copy\n",
    "\n",
    "EVAL_DATA_PATH = \"/Users/james/src/clip/model_eval/dataset-new-apples\"\n",
    "\n",
    "class_names, predictions, model = RoboflowDataLoader(\n",
    "    workspace_url=\"mit-3xwsm\",\n",
    "    project_url=\"appling\",\n",
    "    project_version=1,\n",
    "    image_files=EVAL_DATA_PATH,\n",
    "    model_type=\"classification\",\n",
    ").download_dataset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Comparison\n",
    "\n",
    "Next, we need to choose the prompts we want to evaluate. In the example below, we'll evaluate \"orange\" and \"apple\" to see which one classifies the most images in our dataset correctly.\n",
    "\n",
    "We'll use the prompts to create a list of objects to pass into the `CompareEvaluations` class for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/valid/apple/000325191_jpg.rf.8e478cb228cb5e7a7b17b14e26466968.jpg ... ['orange', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/valid/apple/apple-fuji-1-kg-product-images-o590000001-p590000001-0-202203151906_jpg.rf.f518348005b1d18b089c26bc018a02d2.jpg ... ['orange', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/test/apple/images-1-_jpg.rf.23bed4416b487cf1419b2761f3b6a492.jpg ... ['orange', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/train/apple/GP_74888233_GP_L_jpg.rf.c016cba8bc070a691955b37c0df96b02.jpg ... ['orange', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/train/apple/images_jpg.rf.b21a1b7b5a0ea2c68ac88ca41aa32235.jpg ... ['orange', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/train/apple/20Ounce_NYAS-Apples2_png.rf.3541f74ff8d85a2aeafea45ec366cd4d.jpg ... ['orange', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/train/apple/6000200094512_jpg.rf.dfadbaf4370c70e66fe44304dd9555f3.jpg ... ['orange', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/train/apple/Apple_jpg.rf.30a95a91386b71de8d72ae95b1999f0c.jpg ... ['orange', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/train/apple/apple-fuji-exoticfruitscouk-982233_jpg.rf.c435c84992101c99136132ff25c52a61.jpg ... ['orange', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/train/apple/hot81z-1024x866_jpg.rf.998bd30fb9d0d62bccd3cd0295db1eb8.jpg ... ['orange', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/valid/apple/000325191_jpg.rf.8e478cb228cb5e7a7b17b14e26466968.jpg ... ['red apple', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/valid/apple/apple-fuji-1-kg-product-images-o590000001-p590000001-0-202203151906_jpg.rf.f518348005b1d18b089c26bc018a02d2.jpg ... ['red apple', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/test/apple/images-1-_jpg.rf.23bed4416b487cf1419b2761f3b6a492.jpg ... ['red apple', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/train/apple/GP_74888233_GP_L_jpg.rf.c016cba8bc070a691955b37c0df96b02.jpg ... ['red apple', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/train/apple/images_jpg.rf.b21a1b7b5a0ea2c68ac88ca41aa32235.jpg ... ['red apple', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/train/apple/20Ounce_NYAS-Apples2_png.rf.3541f74ff8d85a2aeafea45ec366cd4d.jpg ... ['red apple', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/train/apple/6000200094512_jpg.rf.dfadbaf4370c70e66fe44304dd9555f3.jpg ... ['red apple', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/train/apple/Apple_jpg.rf.30a95a91386b71de8d72ae95b1999f0c.jpg ... ['red apple', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/train/apple/apple-fuji-exoticfruitscouk-982233_jpg.rf.c435c84992101c99136132ff25c52a61.jpg ... ['red apple', 'background']\n",
      "evaluating image predictions against ground truth /Users/james/src/clip/model_eval/dataset-new-apples/train/apple/hot81z-1024x866_jpg.rf.998bd30fb9d0d62bccd3cd0295db1eb8.jpg ... ['red apple', 'background']\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "f1 Score: 1.0\n",
      "Class Name: red apple\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "evals = [\n",
    "    [\"orange\", \"background\"],\n",
    "    [\"red apple\", \"background\"],\n",
    "]\n",
    "\n",
    "best = CompareEvaluations(\n",
    "    [\n",
    "        CLIPEvaluator(\n",
    "            data=CLIPDataLoader(\n",
    "                data=copy.deepcopy(predictions),\n",
    "                class_names=cn,\n",
    "                eval_data_path=EVAL_DATA_PATH,\n",
    "            ).process_files(),\n",
    "            class_names=cn,\n",
    "            mode=\"batch\",\n",
    "        )\n",
    "        for cn in evals\n",
    "    ]\n",
    ")\n",
    "\n",
    "precision, recall, f1, class_name = best.compare()\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"f1 Score:\", f1)\n",
    "print(\"Class Name:\", class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lacuw5O1OeXl"
   },
   "source": [
    "# Next steps üöÄ\n",
    "\n",
    "Congratulations on completing this notebook! Use the insights you have derived from this notebook to improve your existing model on Roboflow or to find the ideal prompt for zero-shot labelling.\n",
    "\n",
    "## Learning Resources\n",
    "\n",
    "Roboflow has produced many resources that you may find interesting as you advance your knowledge of computer vision:\n",
    "\n",
    "- [Roboflow Notebooks](https://github.com/roboflow/notebooks): A repository of over 20 notebooks that walk through how to train custom models with a range of model types, from YOLOv7 to SegFormer. (This notebook is in the Notebooks repository!)\n",
    "- [Roboflow Supervision](https://github.com/roboflow/supervision): Utilities to implement common computer vision functions into your project, from drawing bounding boxes to counting predictions in specified zones.\n",
    "- [Roboflow YouTube](https://www.youtube.com/c/Roboflow): Our library of videos featuring deep dives into the latest in computer vision, detailed tutorials that accompany our notebooks, and more.\n",
    "- [Roboflow Discuss](https://discuss.roboflow.com/): Have a question about how to do something on Roboflow? Ask your question on our discussion forum.\n",
    "- [Roboflow Models](https://roboflow.com): Learn about state-of-the-art models and their performance. Find links and tutorials to guide your learning.\n",
    "\n",
    "## Connect computer vision to your project logic\n",
    "\n",
    "[Roboflow Templates](https://roboflow.com/templates) is a public gallery of code snippets that you can use to connect computer vision to your project logic. Code snippets range from sending emails after inference to measuring object distance between detections."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
